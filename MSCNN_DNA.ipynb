{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05be4218-b17f-48d6-8f1a-518c7d39410f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05be4218-b17f-48d6-8f1a-518c7d39410f",
    "outputId": "5bf19902-5c9b-4f01-8f30-c382ae511461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth on GPUs\n",
      "Training set: X=(159883, 1, 15, 1024), y=(159883, 2)\n",
      "   Testing set: X=(75258, 1, 15, 1024),  y=(75258, 2)\n",
      "Epoch 1/20\n",
      "4997/4997 [==============================] - 11s 2ms/step - loss: 0.2203 - accuracy: 0.9159\n",
      "Epoch 2/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1984 - accuracy: 0.9221\n",
      "Epoch 3/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1861 - accuracy: 0.9261\n",
      "Epoch 4/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1766 - accuracy: 0.9296\n",
      "Epoch 5/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1644 - accuracy: 0.9336\n",
      "Epoch 6/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1565 - accuracy: 0.9363\n",
      "Epoch 7/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1460 - accuracy: 0.9407\n",
      "Epoch 8/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1384 - accuracy: 0.9429\n",
      "Epoch 9/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1317 - accuracy: 0.9456\n",
      "Epoch 10/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1266 - accuracy: 0.9475\n",
      "Epoch 11/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1229 - accuracy: 0.9492\n",
      "Epoch 12/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1169 - accuracy: 0.9506\n",
      "Epoch 13/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1140 - accuracy: 0.9524\n",
      "Epoch 14/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1118 - accuracy: 0.9533\n",
      "Epoch 15/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1072 - accuracy: 0.9555\n",
      "Epoch 16/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1047 - accuracy: 0.9564\n",
      "Epoch 17/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1025 - accuracy: 0.9577\n",
      "Epoch 18/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1000 - accuracy: 0.9585\n",
      "Epoch 19/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.0963 - accuracy: 0.9597\n",
      "Epoch 20/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.0953 - accuracy: 0.9609\n",
      "2352/2352 [==============================] - 3s 1ms/step\n",
      "Saved ROC data to: dataset\\Roc\\DeepScan_RAG_1772262976.npz\n",
      "\n",
      "=== DeepScan_RAG Evaluation ===\n",
      "Best thresh: 0.0171 (G-Mean=0.8066), AUC=0.8864\n",
      "TP=2586, FP=13903, TN=58147, FN=622\n",
      "Sensitivity=0.8061, Specificity=0.8070\n",
      "Accuracy=0.8070, Precision=0.1568, F1=0.2626, MCC=0.2995\n",
      "\n",
      "Final results: {'TP': 2586, 'FP': 13903, 'TN': 58147, 'FN': 622, 'Sensitivity': 0.8061097256857855, 'Specificity': 0.8070367800138792, 'Accuracy': 0.8069972627494751, 'Precision': 0.1568318272787919, 'F1': 0.26257805757221914, 'MCC': 0.2994519327513624, 'AUC': 0.8863966752964917}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  GPU MEMORY CONFIGURATION\n",
    "# =========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Could not set GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  PARAMETERS\n",
    "# =========================\n",
    "NUM_DEPENDENT = 7\n",
    "MAXSEQ        = NUM_DEPENDENT * 2 + 1  # 15\n",
    "NUM_FEATURE   = 1024\n",
    "NUM_FILTER    = 128\n",
    "NUM_HIDDEN    = 1000\n",
    "BATCH_SIZE    = 32\n",
    "WINDOW_SIZES  = [4, 6, 8, 10, 12]\n",
    "NUM_CLASSES   = 2\n",
    "EPOCHS        = 20\n",
    "\n",
    "# Paths\n",
    "# DATA_DIR     = '/content/drive/MyDrive/s1116049'\n",
    "DATA_DIR     = 'dataset'\n",
    "ROC_SAVE_DIR = os.path.join(DATA_DIR, 'Roc')\n",
    "os.makedirs(ROC_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "#  DATA LOADING\n",
    "# =========================\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'TR573_rag_common_data.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'TR573_label.npy'), allow_pickle=True)\n",
    "x_test  = np.load(os.path.join(DATA_DIR, 'TE181_rag_data.npy'), allow_pickle=True)\n",
    "y_test  = np.load(os.path.join(DATA_DIR, 'TE181_label.npy'), allow_pickle=True)\n",
    "\n",
    "print(f\"Training set: X={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"   Testing set: X={x_test.shape},  y={y_test.shape}\")\n",
    "\n",
    "# =========================\n",
    "#  DATA GENERATOR\n",
    "# =========================\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bs = batch_size\n",
    "        self.indexes = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.bs))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idxs = self.indexes[idx * self.bs:(idx + 1) * self.bs]\n",
    "        return self.X[batch_idxs], self.y[batch_idxs]\n",
    "\n",
    "# =========================\n",
    "#  MODEL DEFINITION\n",
    "# =========================\n",
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=WINDOW_SIZES,\n",
    "                 num_filters=NUM_FILTER,\n",
    "                 num_hidden=NUM_HIDDEN):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = []\n",
    "        self.pools = []\n",
    "        for w in window_sizes:\n",
    "            self.convs.append(\n",
    "                layers.SeparableConv2D(filters=num_filters,\n",
    "                              kernel_size=(1, w),\n",
    "                              activation='relu',\n",
    "                              padding='valid')\n",
    "            )\n",
    "            self.pools.append(\n",
    "                layers.MaxPooling2D(\n",
    "                    pool_size=(1, MAXSEQ - w + 1),\n",
    "                    strides=(1, MAXSEQ)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dropout = layers.Dropout(0.7)\n",
    "        self.dense1  = layers.Dense(num_hidden, activation='relu')\n",
    "        self.dense2  = layers.Dense(NUM_CLASSES, activation='softmax',\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = []\n",
    "        for conv, pool in zip(self.convs, self.pools):\n",
    "            h = conv(x)\n",
    "            h = pool(h)\n",
    "            features.append(self.flatten(h))\n",
    "        x = tf.concat(features, axis=1)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# =========================\n",
    "#  ROC SAVE & EVALUATION\n",
    "# =========================\n",
    "def save_roc_npz(fpr, tpr, thresholds, auc, model_name=\"DeepScan\"):\n",
    "    timestamp = int(time())\n",
    "    filename = os.path.join(ROC_SAVE_DIR, f\"{model_name}_{timestamp}.npz\")\n",
    "    np.savez(\n",
    "        filename,\n",
    "        fpr=fpr,\n",
    "        tpr=tpr,\n",
    "        thresholds=thresholds,\n",
    "        auc=auc\n",
    "    )\n",
    "    print(f\"Saved ROC data to: {filename}\")\n",
    "\n",
    "\n",
    "def model_test(model, X, y, model_name=\"DeepScan\"):\n",
    "    preds = model.predict(X, batch_size=BATCH_SIZE)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[:, 1], preds[:, 1])\n",
    "    auc_value = metrics.auc(fpr, tpr)\n",
    "    save_roc_npz(fpr, tpr, thresholds, auc_value, model_name)\n",
    "\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    best_thresh, best_g = thresholds[ix], gmeans[ix]\n",
    "    y_pred = (preds[:, 1] >= best_thresh).astype(int)\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y[:, 1], y_pred).ravel()\n",
    "\n",
    "    Sens = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    Spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    Acc  = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Prec = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    MCC  = metrics.matthews_corrcoef(y[:, 1], y_pred)\n",
    "    F1   = metrics.f1_score(y[:, 1], y_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Best thresh: {best_thresh:.4f} (G-Mean={best_g:.4f}), AUC={auc_value:.4f}\")\n",
    "    print(f\"TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "    print(f\"Sensitivity={Sens:.4f}, Specificity={Spec:.4f}\")\n",
    "    print(f\"Accuracy={Acc:.4f}, Precision={Prec:.4f}, F1={F1:.4f}, MCC={MCC:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"Sensitivity\": Sens, \"Specificity\": Spec,\n",
    "        \"Accuracy\": Acc, \"Precision\": Prec,\n",
    "        \"F1\": F1, \"MCC\": MCC, \"AUC\": auc_value\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "#  MAIN: TRAIN & EVALUATE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_gen = DataGenerator(x_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    model = DeepScan()\n",
    "    model.build(input_shape=(None, 1, MAXSEQ, NUM_FEATURE))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Free memory\n",
    "    del x_train, y_train, train_gen\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    # Test\n",
    "    results = model_test(model, x_test, y_test, model_name=\"DeepScan_RAG\")\n",
    "    print(\"Final results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98ca618-42a3-436d-8f19-f6676f101c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth on GPUs\n",
      "Training set: X=(159883, 1, 15, 1024), y=(159883, 2)\n",
      "   Testing set: X=(37515, 1, 15, 1024),  y=(37515, 2)\n",
      "Epoch 1/20\n",
      "4997/4997 [==============================] - 11s 2ms/step - loss: 0.2189 - accuracy: 0.9161\n",
      "Epoch 2/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1962 - accuracy: 0.9223\n",
      "Epoch 3/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1853 - accuracy: 0.9267\n",
      "Epoch 4/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1747 - accuracy: 0.9300\n",
      "Epoch 5/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1636 - accuracy: 0.9346\n",
      "Epoch 6/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1548 - accuracy: 0.9373\n",
      "Epoch 7/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1446 - accuracy: 0.9416\n",
      "Epoch 8/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1381 - accuracy: 0.9433\n",
      "Epoch 9/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1303 - accuracy: 0.9459\n",
      "Epoch 10/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1246 - accuracy: 0.9486\n",
      "Epoch 11/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1197 - accuracy: 0.9502\n",
      "Epoch 12/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1147 - accuracy: 0.9518\n",
      "Epoch 13/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1126 - accuracy: 0.9530\n",
      "Epoch 14/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1083 - accuracy: 0.9548\n",
      "Epoch 15/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1047 - accuracy: 0.9566\n",
      "Epoch 16/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1032 - accuracy: 0.9570\n",
      "Epoch 17/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.1003 - accuracy: 0.9584\n",
      "Epoch 18/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.0967 - accuracy: 0.9601\n",
      "Epoch 19/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.0958 - accuracy: 0.9603\n",
      "Epoch 20/20\n",
      "4997/4997 [==============================] - 10s 2ms/step - loss: 0.0926 - accuracy: 0.9613\n",
      "1173/1173 [==============================] - 1s 1ms/step\n",
      "Saved ROC data to: dataset\\Roc\\DeepScan_RAG_1772265238.npz\n",
      "\n",
      "=== DeepScan_RAG Evaluation ===\n",
      "Best thresh: 0.0151 (G-Mean=0.8383), AUC=0.9131\n",
      "TP=1882, FP=5769, TN=29506, FN=358\n",
      "Sensitivity=0.8402, Specificity=0.8365\n",
      "Accuracy=0.8367, Precision=0.2460, F1=0.3805, MCC=0.3979\n",
      "\n",
      "Final results: {'TP': 1882, 'FP': 5769, 'TN': 29506, 'FN': 358, 'Sensitivity': 0.8401785714285714, 'Specificity': 0.8364564138908576, 'Accuracy': 0.8366786618685859, 'Precision': 0.24598091752712065, 'F1': 0.3805479729046608, 'MCC': 0.39790452692835954, 'AUC': 0.9130950693530425}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  GPU MEMORY CONFIGURATION\n",
    "# =========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Could not set GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  PARAMETERS\n",
    "# =========================\n",
    "NUM_DEPENDENT = 7\n",
    "MAXSEQ        = NUM_DEPENDENT * 2 + 1  # 15\n",
    "NUM_FEATURE   = 1024\n",
    "NUM_FILTER    = 128\n",
    "NUM_HIDDEN    = 1000\n",
    "BATCH_SIZE    = 32\n",
    "WINDOW_SIZES  = [4, 6, 8, 10, 12]\n",
    "NUM_CLASSES   = 2\n",
    "EPOCHS        = 20\n",
    "\n",
    "# Paths\n",
    "# DATA_DIR     = '/content/drive/MyDrive/s1116049'\n",
    "DATA_DIR     = 'dataset'\n",
    "ROC_SAVE_DIR = os.path.join(DATA_DIR, 'Roc')\n",
    "os.makedirs(ROC_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "#  DATA LOADING\n",
    "# =========================\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'TR573_rag_common_data.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'TR573_label.npy'), allow_pickle=True)\n",
    "x_test  = np.load(os.path.join(DATA_DIR, 'TE129_rag_data.npy'), allow_pickle=True)\n",
    "y_test  = np.load(os.path.join(DATA_DIR, 'TE129_label.npy'), allow_pickle=True)\n",
    "\n",
    "print(f\"Training set: X={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"   Testing set: X={x_test.shape},  y={y_test.shape}\")\n",
    "\n",
    "# =========================\n",
    "#  DATA GENERATOR\n",
    "# =========================\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bs = batch_size\n",
    "        self.indexes = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.bs))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idxs = self.indexes[idx * self.bs:(idx + 1) * self.bs]\n",
    "        return self.X[batch_idxs], self.y[batch_idxs]\n",
    "\n",
    "# =========================\n",
    "#  MODEL DEFINITION\n",
    "# =========================\n",
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=WINDOW_SIZES,\n",
    "                 num_filters=NUM_FILTER,\n",
    "                 num_hidden=NUM_HIDDEN):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = []\n",
    "        self.pools = []\n",
    "        for w in window_sizes:\n",
    "            self.convs.append(\n",
    "                layers.SeparableConv2D(filters=num_filters,\n",
    "                              kernel_size=(1, w),\n",
    "                              activation='relu',\n",
    "                              padding='valid')\n",
    "            )\n",
    "            self.pools.append(\n",
    "                layers.MaxPooling2D(\n",
    "                    pool_size=(1, MAXSEQ - w + 1),\n",
    "                    strides=(1, MAXSEQ)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dropout = layers.Dropout(0.7)\n",
    "        self.dense1  = layers.Dense(num_hidden, activation='relu')\n",
    "        self.dense2  = layers.Dense(NUM_CLASSES, activation='softmax',\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = []\n",
    "        for conv, pool in zip(self.convs, self.pools):\n",
    "            h = conv(x)\n",
    "            h = pool(h)\n",
    "            features.append(self.flatten(h))\n",
    "        x = tf.concat(features, axis=1)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# =========================\n",
    "#  ROC SAVE & EVALUATION\n",
    "# =========================\n",
    "def save_roc_npz(fpr, tpr, thresholds, auc, model_name=\"DeepScan\"):\n",
    "    timestamp = int(time())\n",
    "    filename = os.path.join(ROC_SAVE_DIR, f\"{model_name}_{timestamp}.npz\")\n",
    "    np.savez(\n",
    "        filename,\n",
    "        fpr=fpr,\n",
    "        tpr=tpr,\n",
    "        thresholds=thresholds,\n",
    "        auc=auc\n",
    "    )\n",
    "    print(f\"Saved ROC data to: {filename}\")\n",
    "\n",
    "\n",
    "def model_test(model, X, y, model_name=\"DeepScan\"):\n",
    "    preds = model.predict(X, batch_size=BATCH_SIZE)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[:, 1], preds[:, 1])\n",
    "    auc_value = metrics.auc(fpr, tpr)\n",
    "    save_roc_npz(fpr, tpr, thresholds, auc_value, model_name)\n",
    "\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    best_thresh, best_g = thresholds[ix], gmeans[ix]\n",
    "    y_pred = (preds[:, 1] >= best_thresh).astype(int)\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y[:, 1], y_pred).ravel()\n",
    "\n",
    "    Sens = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    Spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    Acc  = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Prec = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    MCC  = metrics.matthews_corrcoef(y[:, 1], y_pred)\n",
    "    F1   = metrics.f1_score(y[:, 1], y_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Best thresh: {best_thresh:.4f} (G-Mean={best_g:.4f}), AUC={auc_value:.4f}\")\n",
    "    print(f\"TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "    print(f\"Sensitivity={Sens:.4f}, Specificity={Spec:.4f}\")\n",
    "    print(f\"Accuracy={Acc:.4f}, Precision={Prec:.4f}, F1={F1:.4f}, MCC={MCC:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"Sensitivity\": Sens, \"Specificity\": Spec,\n",
    "        \"Accuracy\": Acc, \"Precision\": Prec,\n",
    "        \"F1\": F1, \"MCC\": MCC, \"AUC\": auc_value\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "#  MAIN: TRAIN & EVALUATE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_gen = DataGenerator(x_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    model = DeepScan()\n",
    "    model.build(input_shape=(None, 1, MAXSEQ, NUM_FEATURE))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Free memory\n",
    "    del x_train, y_train, train_gen\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    # Test\n",
    "    results = model_test(model, x_test, y_test, model_name=\"DeepScan_RAG\")\n",
    "    print(\"Final results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4834015e-4a58-4528-a1bb-50db89ff08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth on GPUs\n",
      "Training set: X=(314139, 1, 15, 1024), y=(314139, 2)\n",
      "   Testing set: X=(10876, 1, 15, 1024),  y=(10876, 2)\n",
      "Epoch 1/20\n",
      "9817/9817 [==============================] - 21s 2ms/step - loss: 0.1295 - accuracy: 0.9520\n",
      "Epoch 2/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.1175 - accuracy: 0.9554\n",
      "Epoch 3/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.1112 - accuracy: 0.9573\n",
      "Epoch 4/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.1063 - accuracy: 0.9583\n",
      "Epoch 5/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.1026 - accuracy: 0.9598\n",
      "Epoch 6/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0982 - accuracy: 0.9608\n",
      "Epoch 7/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0935 - accuracy: 0.9623\n",
      "Epoch 8/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0899 - accuracy: 0.9634\n",
      "Epoch 9/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0868 - accuracy: 0.9650\n",
      "Epoch 10/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0842 - accuracy: 0.9655\n",
      "Epoch 11/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0803 - accuracy: 0.9668\n",
      "Epoch 12/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0778 - accuracy: 0.9682\n",
      "Epoch 13/20\n",
      "9817/9817 [==============================] - 21s 2ms/step - loss: 0.0764 - accuracy: 0.9683\n",
      "Epoch 14/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0751 - accuracy: 0.9694\n",
      "Epoch 15/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0729 - accuracy: 0.9698\n",
      "Epoch 16/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0717 - accuracy: 0.9704\n",
      "Epoch 17/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0683 - accuracy: 0.9716\n",
      "Epoch 18/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0680 - accuracy: 0.9718\n",
      "Epoch 19/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0657 - accuracy: 0.9730\n",
      "Epoch 20/20\n",
      "9817/9817 [==============================] - 20s 2ms/step - loss: 0.0652 - accuracy: 0.9732\n",
      "340/340 [==============================] - 0s 1ms/step\n",
      "Saved ROC data to: dataset\\Roc\\DeepScan_RAG_1772255552.npz\n",
      "\n",
      "=== DeepScan_RAG Evaluation ===\n",
      "Best thresh: 0.0446 (G-Mean=0.8093), AUC=0.8833\n",
      "TP=778, FP=1859, TN=8052, FN=187\n",
      "Sensitivity=0.8062, Specificity=0.8124\n",
      "Accuracy=0.8119, Precision=0.2950, F1=0.4320, MCC=0.4105\n",
      "\n",
      "Final results: {'TP': 778, 'FP': 1859, 'TN': 8052, 'FN': 187, 'Sensitivity': 0.8062176165803109, 'Specificity': 0.8124306326304107, 'Accuracy': 0.8118793674144906, 'Precision': 0.2950322335987865, 'F1': 0.4319822320932815, 'MCC': 0.41046285975768865, 'AUC': 0.8832644212245461}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  GPU MEMORY CONFIGURATION\n",
    "# =========================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Could not set GPU memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected, running on CPU.\")\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  PARAMETERS\n",
    "# =========================\n",
    "NUM_DEPENDENT = 7\n",
    "MAXSEQ        = NUM_DEPENDENT * 2 + 1  # 15\n",
    "NUM_FEATURE   = 1024\n",
    "NUM_FILTER    = 128\n",
    "NUM_HIDDEN    = 1000\n",
    "BATCH_SIZE    = 32\n",
    "WINDOW_SIZES  = [4, 6, 8, 10, 12]\n",
    "NUM_CLASSES   = 2\n",
    "EPOCHS        = 20\n",
    "\n",
    "# Paths\n",
    "# DATA_DIR     = '/content/drive/MyDrive/s1116049'\n",
    "DATA_DIR     = 'dataset'\n",
    "ROC_SAVE_DIR = os.path.join(DATA_DIR, 'Roc')\n",
    "os.makedirs(ROC_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "#  DATA LOADING\n",
    "# =========================\n",
    "x_train = np.load(os.path.join(DATA_DIR, 'TR646_rag_common_data.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'TR646_label.npy'), allow_pickle=True)\n",
    "x_test  = np.load(os.path.join(DATA_DIR, 'TE46_rag_data.npy'), allow_pickle=True)\n",
    "y_test  = np.load(os.path.join(DATA_DIR, 'TE46_label.npy'), allow_pickle=True)\n",
    "\n",
    "print(f\"Training set: X={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"   Testing set: X={x_test.shape},  y={y_test.shape}\")\n",
    "\n",
    "# =========================\n",
    "#  DATA GENERATOR\n",
    "# =========================\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bs = batch_size\n",
    "        self.indexes = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.bs))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idxs = self.indexes[idx * self.bs:(idx + 1) * self.bs]\n",
    "        return self.X[batch_idxs], self.y[batch_idxs]\n",
    "\n",
    "# =========================\n",
    "#  MODEL DEFINITION\n",
    "# =========================\n",
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=WINDOW_SIZES,\n",
    "                 num_filters=NUM_FILTER,\n",
    "                 num_hidden=NUM_HIDDEN):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = []\n",
    "        self.pools = []\n",
    "        for w in window_sizes:\n",
    "            self.convs.append(\n",
    "                layers.SeparableConv2D(filters=num_filters,\n",
    "                              kernel_size=(1, w),\n",
    "                              activation='relu',\n",
    "                              padding='valid')\n",
    "            )\n",
    "            self.pools.append(\n",
    "                layers.MaxPooling2D(\n",
    "                    pool_size=(1, MAXSEQ - w + 1),\n",
    "                    strides=(1, MAXSEQ)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dropout = layers.Dropout(0.7)\n",
    "        self.dense1  = layers.Dense(num_hidden, activation='relu')\n",
    "        self.dense2  = layers.Dense(NUM_CLASSES, activation='softmax',\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = []\n",
    "        for conv, pool in zip(self.convs, self.pools):\n",
    "            h = conv(x)\n",
    "            h = pool(h)\n",
    "            features.append(self.flatten(h))\n",
    "        x = tf.concat(features, axis=1)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# =========================\n",
    "#  ROC SAVE & EVALUATION\n",
    "# =========================\n",
    "def save_roc_npz(fpr, tpr, thresholds, auc, model_name=\"DeepScan\"):\n",
    "    timestamp = int(time())\n",
    "    filename = os.path.join(ROC_SAVE_DIR, f\"{model_name}_{timestamp}.npz\")\n",
    "    np.savez(\n",
    "        filename,\n",
    "        fpr=fpr,\n",
    "        tpr=tpr,\n",
    "        thresholds=thresholds,\n",
    "        auc=auc\n",
    "    )\n",
    "    print(f\"Saved ROC data to: {filename}\")\n",
    "\n",
    "\n",
    "def model_test(model, X, y, model_name=\"DeepScan\"):\n",
    "    preds = model.predict(X, batch_size=BATCH_SIZE)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[:, 1], preds[:, 1])\n",
    "    auc_value = metrics.auc(fpr, tpr)\n",
    "    save_roc_npz(fpr, tpr, thresholds, auc_value, model_name)\n",
    "\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    best_thresh, best_g = thresholds[ix], gmeans[ix]\n",
    "    y_pred = (preds[:, 1] >= best_thresh).astype(int)\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y[:, 1], y_pred).ravel()\n",
    "\n",
    "    Sens = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    Spec = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    Acc  = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Prec = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    MCC  = metrics.matthews_corrcoef(y[:, 1], y_pred)\n",
    "    F1   = metrics.f1_score(y[:, 1], y_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Best thresh: {best_thresh:.4f} (G-Mean={best_g:.4f}), AUC={auc_value:.4f}\")\n",
    "    print(f\"TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "    print(f\"Sensitivity={Sens:.4f}, Specificity={Spec:.4f}\")\n",
    "    print(f\"Accuracy={Acc:.4f}, Precision={Prec:.4f}, F1={F1:.4f}, MCC={MCC:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"Sensitivity\": Sens, \"Specificity\": Spec,\n",
    "        \"Accuracy\": Acc, \"Precision\": Prec,\n",
    "        \"F1\": F1, \"MCC\": MCC, \"AUC\": auc_value\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "#  MAIN: TRAIN & EVALUATE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_gen = DataGenerator(x_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    model = DeepScan()\n",
    "    model.build(input_shape=(None, 1, MAXSEQ, NUM_FEATURE))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Free memory\n",
    "    del x_train, y_train, train_gen\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    # Test\n",
    "    results = model_test(model, x_test, y_test, model_name=\"DeepScan_RAG\")\n",
    "    print(\"Final results:\", results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
