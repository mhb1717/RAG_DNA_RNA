{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bfb95-7b7d-40e8-9c8d-bb48cfc3cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# =========================\n",
    "#  GPU MEMORY CONFIGURATION\n",
    "# =========================\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Could not set GPU memory growth:\", e)\n",
    "\n",
    "# =========================\n",
    "#  PARAMETERS\n",
    "# =========================\n",
    "NUM_DEPENDENT  = 7\n",
    "MAXSEQ         = NUM_DEPENDENT * 2 + 1\n",
    "NUM_FEATURE    = 1024\n",
    "NUM_FILTER     = 128\n",
    "NUM_HIDDEN     = 1000\n",
    "BATCH_SIZE     = 32\n",
    "WINDOW_SIZES   = [4, 6, 8, 10, 12]\n",
    "NUM_CLASSES    = 2\n",
    "EPOCHS         = 10\n",
    "\n",
    "# Where to save your ROC curves\n",
    "ROC_SAVE_DIR   = \"C:/jupyter/Malik/hERG/Hussain/DNA/output/Roc\"\n",
    "os.makedirs(ROC_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "#  DATA LOADING\n",
    "# =========================\n",
    "x_train = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/output/TR573_RAG.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/output/TR573_label.npy\", allow_pickle=True)\n",
    "x_test  = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/output/TE129_RAG.npy\", allow_pickle=True)\n",
    "y_test  = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/output/TE129_label.npy\", allow_pickle=True)\n",
    "\n",
    "print(f\"Training set: X={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"   Testing set: X={x_test.shape},  y={y_test.shape}\")\n",
    "\n",
    "# =========================\n",
    "#  DATA GENERATOR\n",
    "# =========================\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bs = batch_size\n",
    "        self.indexes = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.X) / self.bs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idxs = self.indexes[idx * self.bs:(idx + 1) * self.bs]\n",
    "        return self.X[batch_idxs], self.y[batch_idxs]\n",
    "\n",
    "# =========================\n",
    "#  MODEL DEFINITION\n",
    "# =========================\n",
    "class DeepScan(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=WINDOW_SIZES,\n",
    "                 num_filters=NUM_FILTER,\n",
    "                 num_hidden=NUM_HIDDEN):\n",
    "        super().__init__()\n",
    "\n",
    "        # Depthwise Separable Convolution + Pooling for each window size\n",
    "        self.convs = []\n",
    "        self.pools = []\n",
    "        for w in window_sizes:\n",
    "            self.convs.append(layers.SeparableConv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=(1, w),\n",
    "                activation='relu',\n",
    "                padding='valid'\n",
    "            ))\n",
    "            self.pools.append(layers.MaxPooling2D(\n",
    "                pool_size=(1, MAXSEQ - w + 1),\n",
    "                strides=(1, MAXSEQ - w + 1)\n",
    "            ))\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dropout = layers.Dropout(0.7)\n",
    "        self.dense1  = layers.Dense(num_hidden, activation='relu')\n",
    "        self.dense2  = layers.Dense(NUM_CLASSES, activation='softmax',\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = []\n",
    "        for conv, pool in zip(self.convs, self.pools):\n",
    "            h = conv(x)\n",
    "            h = pool(h)\n",
    "            features.append(self.flatten(h))\n",
    "        x = tf.concat(features, axis=1)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# =========================\n",
    "#  ROC SAVE & EVALUATION\n",
    "# =========================\n",
    "def save_roc(fpr, tpr, auc, model_name=\"DeepScan\"):\n",
    "    timestamp = int(time())\n",
    "    filename = os.path.join(ROC_SAVE_DIR, f\"{model_name}_{timestamp}.pkl\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({\"fpr\": fpr, \"tpr\": tpr, \"auc\": auc}, f)\n",
    "    print(f\"Saved ROC data to: {filename}\")\n",
    "\n",
    "def model_test(model, X, y, model_name=\"DeepScan\"):\n",
    "    preds = model.predict(X, batch_size=BATCH_SIZE)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[:, 1], preds[:, 1])\n",
    "    auc_value = metrics.auc(fpr, tpr)\n",
    "    save_roc(fpr, tpr, auc_value, model_name)\n",
    "\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    best_thresh, best_g = thresholds[ix], gmeans[ix]\n",
    "\n",
    "    y_pred = (preds[:, 1] >= best_thresh).astype(int)\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y[:, 1], y_pred).ravel()\n",
    "\n",
    "    Sens  = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    Spec  = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    Acc   = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Prec  = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    MCC   = metrics.matthews_corrcoef(y[:, 1], y_pred)\n",
    "    F1    = metrics.f1_score(y[:, 1], y_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Best thresh: {best_thresh:.4f} (G-Mean={best_g:.4f}), AUC={auc_value:.4f}\")\n",
    "    print(f\"TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "    print(f\"Sensitivity={Sens:.4f}, Specificity={Spec:.4f}\")\n",
    "    print(f\"Accuracy={Acc:.4f}, Precision={Prec:.4f}, F1={F1:.4f}, MCC={MCC:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"Sensitivity\": Sens, \"Specificity\": Spec,\n",
    "        \"Accuracy\": Acc, \"Precision\": Prec,\n",
    "        \"F1\": F1, \"MCC\": MCC, \"AUC\": auc_value\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "#  TRAIN & EVALUATE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_gen = DataGenerator(x_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    model = DeepScan()\n",
    "    model.build(input_shape=x_train.shape)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Free memory\n",
    "    del x_train, y_train, train_gen\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    results = model_test(model, x_test, y_test, model_name=\"DeepScan_RAG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff2cb3a-98d3-4ec3-b1db-9a8916238ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled memory growth on GPUs\n",
      "Training set: X=(32071, 1, 15, 1024), y=(32071, 2)\n",
      "Testing set: X=(4424, 1, 15, 1024), y=(4424, 2)\n",
      "Model: \"deep_scan_ssm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " separable_conv2d (Separable  multiple                 135296    \n",
      " Conv2D)                                                         \n",
      "                                                                 \n",
      " separable_conv2d_1 (Separab  multiple                 137344    \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_2 (Separab  multiple                 139392    \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_3 (Separab  multiple                 141440    \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " separable_conv2d_4 (Separab  multiple                 143488    \n",
      " leConv2D)                                                       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  multiple                 0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " state_space_layer (StateSpa  multiple                 443136    \n",
      " ceLayer)                                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  769000    \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  2002      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,911,098\n",
      "Trainable params: 1,911,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1003/1003 [==============================] - 9s 3ms/step - loss: 0.1440 - accuracy: 0.9476\n",
      "Epoch 2/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0766 - accuracy: 0.9763\n",
      "Epoch 3/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0596 - accuracy: 0.9805\n",
      "Epoch 4/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0479 - accuracy: 0.9844\n",
      "Epoch 5/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0386 - accuracy: 0.9872\n",
      "Epoch 6/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0342 - accuracy: 0.9880\n",
      "Epoch 7/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0296 - accuracy: 0.9887\n",
      "Epoch 8/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0232 - accuracy: 0.9913\n",
      "Epoch 9/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0221 - accuracy: 0.9918\n",
      "Epoch 10/10\n",
      "1003/1003 [==============================] - 3s 3ms/step - loss: 0.0212 - accuracy: 0.9928\n",
      "139/139 [==============================] - 0s 1ms/step\n",
      "Saved ROC data to: C:/jupyter/Malik/hERG/Hussain/DNA/output/Roc\\DeepScanSSM_1747996383.pkl\n",
      "\n",
      "=== DeepScanSSM Evaluation ===\n",
      "Best thresh: 0.1747 (G-Mean=0.9364), AUC=0.9778\n",
      "TP=1853, FP=117, TN=2296, FN=158\n",
      "Sensitivity=0.9214, Specificity=0.9515\n",
      "Accuracy=0.9378, Precision=0.9406, F1=0.9309, MCC=0.8746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# =========================\n",
    "#  GPU MEMORY CONFIGURATION\n",
    "# =========================\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Enabled memory growth on GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Could not set GPU memory growth:\", e)\n",
    "\n",
    "# =========================\n",
    "#  PARAMETERS\n",
    "# =========================\n",
    "NUM_DEPENDENT  = 7\n",
    "MAXSEQ         = NUM_DEPENDENT * 2 + 1\n",
    "NUM_FEATURE    = 1024\n",
    "NUM_FILTER     = 128\n",
    "NUM_HIDDEN     = 1000\n",
    "BATCH_SIZE     = 32\n",
    "WINDOW_SIZES   = [4, 6, 8, 10, 12]\n",
    "NUM_CLASSES    = 2\n",
    "EPOCHS         = 10\n",
    "\n",
    "ROC_SAVE_DIR   = \"C:/jupyter/Malik/hERG/Hussain/DNA/output/Roc\"\n",
    "os.makedirs(ROC_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "#  DATA LOADING\n",
    "# =========================\n",
    "x_train = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/TR495_data.npy\", allow_pickle=True)\n",
    "y_train = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/TR495_label.npy\", allow_pickle=True)\n",
    "x_test  = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/TE117_data.npy\", allow_pickle=True)\n",
    "y_test  = np.load(\"C:/jupyter/Malik/hERG/Hussain/DNA/TE117_label.npy\", allow_pickle=True)\n",
    "\n",
    "print(f\"Training set: X={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Testing set: X={x_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# =========================\n",
    "#  DATA GENERATOR\n",
    "# =========================\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, X, y, batch_size):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bs = batch_size\n",
    "        self.indexes = np.arange(len(X))\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.X) / self.bs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idxs = self.indexes[idx * self.bs:(idx + 1) * self.bs]\n",
    "        return self.X[batch_idxs], self.y[batch_idxs]\n",
    "\n",
    "# =========================\n",
    "#  STATE SPACE LAYER\n",
    "# =========================\n",
    "class StateSpaceLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(StateSpaceLayer, self).__init__()\n",
    "        self.gru = tf.keras.layers.GRU(units, return_sequences=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.squeeze(x, axis=1)  # from (B, 1, seq, feat) to (B, seq, feat)\n",
    "        return self.gru(x)         # (B, units)\n",
    "\n",
    "# =========================\n",
    "#  DEEPSCAN + SSM MODEL\n",
    "# =========================\n",
    "class DeepScanSSM(Model):\n",
    "    def __init__(self,\n",
    "                 input_shape=(1, MAXSEQ, NUM_FEATURE),\n",
    "                 window_sizes=WINDOW_SIZES,\n",
    "                 num_filters=NUM_FILTER,\n",
    "                 num_hidden=NUM_HIDDEN):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = []\n",
    "        self.pools = []\n",
    "        for w in window_sizes:\n",
    "            self.convs.append(layers.SeparableConv2D(\n",
    "                filters=num_filters,\n",
    "                kernel_size=(1, w),\n",
    "                activation='relu',\n",
    "                padding='valid'\n",
    "            ))\n",
    "            self.pools.append(layers.MaxPooling2D(\n",
    "                pool_size=(1, MAXSEQ - w + 1),\n",
    "                strides=(1, MAXSEQ - w + 1)\n",
    "            ))\n",
    "\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.ssm     = StateSpaceLayer(units=128)\n",
    "        self.dropout = layers.Dropout(0.7)\n",
    "        self.dense1  = layers.Dense(num_hidden, activation='relu')\n",
    "        self.dense2  = layers.Dense(NUM_CLASSES, activation='softmax',\n",
    "                                    kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        features = []\n",
    "        for conv, pool in zip(self.convs, self.pools):\n",
    "            h = conv(x)\n",
    "            h = pool(h)\n",
    "            features.append(self.flatten(h))\n",
    "        cnn_output = tf.concat(features, axis=1)\n",
    "        ssm_output = self.ssm(x)\n",
    "        x = tf.concat([cnn_output, ssm_output], axis=1)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# =========================\n",
    "#  ROC SAVE & EVALUATION\n",
    "# =========================\n",
    "def save_roc(fpr, tpr, auc, model_name=\"DeepScan\"):\n",
    "    timestamp = int(time())\n",
    "    filename = os.path.join(ROC_SAVE_DIR, f\"{model_name}_{timestamp}.pkl\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({\"fpr\": fpr, \"tpr\": tpr, \"auc\": auc}, f)\n",
    "    print(f\"Saved ROC data to: {filename}\")\n",
    "\n",
    "def model_test(model, X, y, model_name=\"DeepScan\"):\n",
    "    preds = model.predict(X, batch_size=BATCH_SIZE)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y[:, 1], preds[:, 1])\n",
    "    auc_value = metrics.auc(fpr, tpr)\n",
    "    save_roc(fpr, tpr, auc_value, model_name)\n",
    "\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    best_thresh, best_g = thresholds[ix], gmeans[ix]\n",
    "\n",
    "    y_pred = (preds[:, 1] >= best_thresh).astype(int)\n",
    "    TN, FP, FN, TP = metrics.confusion_matrix(y[:, 1], y_pred).ravel()\n",
    "\n",
    "    Sens  = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    Spec  = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    Acc   = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Prec  = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    MCC   = metrics.matthews_corrcoef(y[:, 1], y_pred)\n",
    "    F1    = metrics.f1_score(y[:, 1], y_pred)\n",
    "\n",
    "    print(f\"\\n=== {model_name} Evaluation ===\")\n",
    "    print(f\"Best thresh: {best_thresh:.4f} (G-Mean={best_g:.4f}), AUC={auc_value:.4f}\")\n",
    "    print(f\"TP={TP}, FP={FP}, TN={TN}, FN={FN}\")\n",
    "    print(f\"Sensitivity={Sens:.4f}, Specificity={Spec:.4f}\")\n",
    "    print(f\"Accuracy={Acc:.4f}, Precision={Prec:.4f}, F1={F1:.4f}, MCC={MCC:.4f}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"TN\": TN, \"FN\": FN,\n",
    "        \"Sensitivity\": Sens, \"Specificity\": Spec,\n",
    "        \"Accuracy\": Acc, \"Precision\": Prec,\n",
    "        \"F1\": F1, \"MCC\": MCC, \"AUC\": auc_value\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "#  TRAIN & EVALUATE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    train_gen = DataGenerator(x_train, y_train, BATCH_SIZE)\n",
    "\n",
    "    model = DeepScanSSM()\n",
    "    model.build(input_shape=x_train.shape)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Free memory\n",
    "    del x_train, y_train, train_gen\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    results = model_test(model, x_test, y_test, model_name=\"DeepScanSSM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95457e06-2ebd-4ab2-82b3-dbba756d0653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
